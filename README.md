# Full Implementations for CS336 Spring 2025 Assignment 1: Basics

ğŸš€ **Complete Implementation Ready!** The scripts have passed all the unit tests.

ğŸ“š **Comprehensive Documentation**: Each section includes detailed explanations, implementation insights, and answers to common questions with AI help.

ğŸ“– **Section-by-Section Guides**:
- **[ğŸ”¤ BPE Tokenizer](./2-BPE-tokenizer.md)** - Byte-Pair Encoding implementation with training and vocabulary management
- **[ğŸ§  Transformer Architecture](./3-transformer.md)** - Complete transformer implementation with attention, feed-forward, and positional encoding
- **[âš¡ Optimizer & Training](./4-optimizer.md)** - AdamW optimizer, learning rate scheduling, and gradient clipping
- **[ğŸ¯ Model Training](./5-model-training.md)** - Full training pipeline with data loading, checkpointing, and monitoring
- **[âœ¨ Text Generation](./6-text-generation.md)** - Advanced text generation with temperature scaling and nucleus sampling


For a full description of the assignment, see the assignment handout at
[cs336_spring2025_assignment1_basics.pdf](./cs336_spring2025_assignment1_basics.pdf)

If you see any issues with the assignment handout or code, please feel free to
raise a GitHub issue or open a pull request with a fix.

## 1. Setup

### Environment
We manage our environments with `uv` to ensure reproducibility, portability, and ease of use.
Install `uv` [here](https://github.com/astral-sh/uv) (recommended), or run `pip install uv`/`brew install uv`.
We recommend reading a bit about managing projects in `uv` [here](https://docs.astral.sh/uv/guides/projects/#managing-dependencies) (you will not regret it!).

You can now run any code in the repo using
```sh
uv run <python_file_path>
```
and the environment will be automatically solved and activated when necessary.

### Run unit tests


```sh
uv run pytest
```

Initially, all tests should fail with `NotImplementedError`s.
To connect your implementation to the tests, complete the
functions in [./tests/adapters.py](./tests/adapters.py).

### Download data
Download the TinyStories data and a subsample of OpenWebText

``` sh
mkdir -p data
cd data

wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-train.txt
wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStoriesV2-GPT4-valid.txt

wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_train.txt.gz
gunzip owt_train.txt.gz
wget https://huggingface.co/datasets/stanford-cs336/owt-sample/resolve/main/owt_valid.txt.gz
gunzip owt_valid.txt.gz

cd ..
```

## 2. Byte-Pair Encoding (BPE) Tokenizer

- **ğŸš€ Automatic Strategy Selection**: Optimizes for speed and memory based on corpus size.
- **âš™ï¸ Parallel Processing**: Leverages multiple CPU cores for blazing-fast tokenization on large files.
- **ğŸ§  Memory-Efficient**: Includes a streaming mode to handle massive datasets (>5GB) that don't fit in RAM.
- **âœ¨ Special Token Aware**: Correctly handles and preserves special tokens during both training and encoding.

```
uv run pytest tests/test_train_bpe.py
uv run pytest tests/test_tokenizer.py
```

## 3. Transformer Language Model Architecture

- **ğŸ§© Modular Components**: Clean implementation of all core building blocks: `Linear`, `Embedding`, `RMSNorm`, and `SwiGLU`.
- **ğŸŒ€ Rotary Positional Embeddings (RoPE)**: Implements modern relative position encoding for better long-range dependency modeling.
- **âš¡ï¸ Scaled Dot-Product Attention**: Efficient and stable attention mechanism.
- **ğŸ—ï¸ Full Architecture**: Combines all components into a complete `TransformerLM` ready for training.

```
uv run pytest -k test_linear
uv run pytest -k test_embedding
uv run pytest -k test_rmsnorm
uv run pytest -k test_swiglu
uv run pytest -k test_rope
uv run pytest -k test_softmax_matches_pytorch
uv run pytest -k test_4d_scaled_dot_product_attention
uv run pytest -k test_multihead_self_attention
uv run pytest -k test_transformer_block
uv run pytest -k test_transformer_lm
```

## 4. Optimizers and Training Utilities
- **âš™ï¸ AdamW Optimizer**: A from-scratch implementation of the AdamW optimizer with decoupled weight decay.
- **ğŸ“‰ Learning Rate Scheduling**: Implements a cosine decay schedule with a linear warmup phase for stable and effective training.
- **ğŸ”’ Gradient Clipping**: Includes gradient norm clipping to prevent exploding gradients and stabilize training.
```
uv run pytest -k test_adamw
uv run pytest -k test_get_lr_cosine_schedule
uv run pytest -k test_gradient_clipping
```

## 5. Model Training & 6. Text Generation

- **ğŸ’¾ Memory-Mapped Data Loading**: Efficiently handles huge datasets that don't fit in RAM using `np.memmap`.
- **ğŸ”„ Robust Checkpointing**: Save and resume training state (model, optimizer, iteration) to handle interruptions.
- **ğŸ”¥ Advanced Text Generation**: Includes **Temperature Scaling** and **Nucleus (Top-p) Sampling** for fine-grained control over text creativity and quality.
- **ğŸš€ End-to-End Test Script**: A comprehensive test script (`run_model_train.py`) verifies the entire pipeline from training to generation.

```
uv run pytest -k test_get_batch
uv run pytest -k test_checkpointing
```

You can run the `run_model_train.py` script for a quick, comprehensive test of the model training and text generation pipeline.

```
# Run a quick test on CPU
uv run python cs336_basics/run_model_train.py --device cpu --quick

# Test on GPU or Apple Silicon
uv run python cs336_basics/run_model_train.py --device cuda
uv run python cs336_basics/run_model_train.py --device mps
```